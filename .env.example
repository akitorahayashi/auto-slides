# LLM Settings
# Use mock client for development/testing (true/false)
DEBUG=true

# Demo mode - use fixed responses for consistent demos (true/false)
USE_DEMO_RESPONSES=true

# For production with real Ollama API
# DEBUG=false
# USE_DEMO_RESPONSES=false
# OLM_API_ENDPOINT=http://localhost:8000
# OLLAMA_MODEL=qwen3:0.6b

# LLM Processing Settings
USE_TWO_STAGE_LLM=true